{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"scholar_multisearch.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"gAl-djQAnjKc","colab_type":"text"},"source":["### Multi-page, Multi-term Web Scraper For Google Scholar version 6\n","* Designed to scrape all results of Google Scholar searches, up to Scholar's imposed maximum of 100 pages (1000 results) for each search.\n","* Developed using the beautifulsoup and pandas packages; it also requires the requests and time packages.\n","* This code extends a single-page scraping architecture for Google.com search results developed by Edmund Martin, whose original work is available [here](https://edmundmartin.com/scraping-google-with-python/).\n","* Adaptation for Google Scholar, iteration over pages, data extraction and manipulation, and export formatting were coded by Cory J. Combs.\n","\n","#### This scraper consists of five components:\n","1. A user agent, which provides identifying information to the server\n","2. A function to fetch results\n","3. A function to parse results\n","4. An function to execute fetching and parsing with error handlers\n","5. The main search script, which:\n","  * executes the search with the input parameters,\n","  * outputs the results in a pandas data frame,\n","  * extracts metadata elements not consistently identifiable through Google Scholar's html or xml alone,\n","  * cleans and formats the data, and\n","  * exports the fully formatted dataframe into Excel\n","\n","The results may be explored in the output Excel file or in Python using pandas. The final formatted pandas data frame is called \"data_df_clean\"."]},{"cell_type":"code","metadata":{"id":"k69M0wddnjKe","colab_type":"code","colab":{}},"source":["# Imports\n","import requests\n","from bs4 import BeautifulSoup\n","import time\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLQrKEnqnjKh","colab_type":"code","colab":{}},"source":["# Build the user agent\n","USER_AGENT = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KpKkVbBenjKj","colab_type":"code","colab":{}},"source":["# Create function to fetch Google Scholar results given specified search term\n","def fetch_results(search_term, language_code):\n","    \"\"\"Fetch Google Scholar search results given specified search term and language code.\"\"\"\n","    \n","    # Confirm search term is a string, else raise Assertion Error\n","    assert isinstance(search_term, str), 'Search term must be a string'\n","    escaped_search_term = search_term.replace(' ', '+')\n","    \n","    # Establish template URL\n","    scholar_url = 'https://scholar.google.com/scholar?start={}0&q={}&hl={}&as_sdt=1,21&as_ylo={}&as_yhi={}'.format(pagination, escaped_search_term, language_code, from_year, to_year)\n","    \n","    # Response handling\n","    response = requests.get(scholar_url, headers=USER_AGENT)\n","    response.raise_for_status()\n","    \n","    return search_term, response.text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sr-gP2l2njKl","colab_type":"code","colab":{}},"source":["# Create function to parse results\n","def parse_results(html, keyword):\n","    \"\"\"Parses Google Scholar results and appends valid entries to results list.\"\"\"\n","    \n","    # Call html parser\n","    soup = BeautifulSoup(html, 'html.parser')\n","    \n","    # Setup\n","    found_results = [] # Initialize temporary results storage\n","    rank = 1  # Initialize rank, which records position on the search results page (1 <= rank <= 10)\n","    \n","    # Make soup: parse and identify all relevant elements\n","    result_block = soup.find_all('div', attrs={'class': 'gs_ri'})\n","    \n","    # Make the soup beautiful: parse and order the results\n","    for result in result_block:\n","        \n","        # Identify key data\n","        title = result.find('h3')\n","        link = result.find('a', href=True)\n","        metadata = result.find('div', attrs={'class': 'gs_a'})\n","        description = result.find('div', attrs={'class': 'gs_rs'})\n","        \n","        # If both link and title are present, get result data and append as new entry\n","        if link and title:\n","            link = link['href']\n","            title = title.get_text()\n","            if description:\n","                description = description.get_text()\n","            if link != '#':\n","                found_results.append({'Keyword': keyword,\n","                                      'Page': pagination + 1, 'Rank': rank,\n","                                      'Title': title, 'Metadata': metadata,\n","                                      'Description': description, 'Link': link})\n","                if verbose == \"yes\":\n","                    print(\"* New result added...\")\n","                \n","                # Pause the search for for seconds each entry to limit server load \n","                time.sleep(2)\n","                \n","                rank += 1\n","                \n","    return found_results"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2zvIwBX1njKn","colab_type":"code","colab":{}},"source":["# Create scraping function and establish error handling mechanisms\n","def scrape_google(search_term, language_code):\n","    \"\"\"Scraping function with user and server-side error handling.\n","    \n","    Flags incorrect arguments, Google Scholar server blocks, and disconnection.\"\"\"\n"," \n","    try:\n","        keyword, html = fetch_results(search_term, language_code)  # Fetch search results\n","        results = parse_results(html, keyword)  # Parse fetched results\n","        return results\n","    except AssertionError:\n","        raise Exception(\"Incorrect arguments were parsed to the function; please revisit your inputs\")\n","    except requests.HTTPError:\n","        raise Exception(\"Google may have blocked the search; if the problem persists, try running the search from a different IP address, e.g. by connecting to a different network, resetting your router, or using a VPN\")\n","    except requests.RequestException:\n","        raise Exception(\"Connection issue detected; please check your internet connection\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"hunhp1SpnjKo","colab_type":"code","colab":{}},"source":["# Implement search from script execution\n","if __name__ == '__main__':\n","    \n","    # Select parameters - use standard Google Scholar search syntax within single parentheses,\n","    # separating distinct searches with commas\n","    keywords = ['\"life cycle assessment\" OR LCA, electricity, temporal']\n","    from_year = 2009  # Start year/lower bound\n","    to_year = 2009  # End year/upper bound\n","    language_code = \"en\"  # English-language results only\n","    verbose = \"yes\"  # Enter \"yes\" to show addition of each result; any other value turns this off\n","    \n","    # Setup\n","    page_max = 100 # Number of pages from start pagination\n","    # Google Scholar only shows 100 pages; iterating over empty pages will simply add no results\n","    data = []\n","    \n","    # Initialize Search\n","    print(\"Initiating search\")\n","    print(\"-----------------\")\n","    for keyword in keywords:\n","        pagination = 79\n","        print(\"Now searching for\", keyword)\n","        for n in range (pagination, pagination + page_max):\n","            # Note that we do not need to add one to page_max, as the first URL count starts from 0\n","            # and the final page URL counts from 990 (page 99), not 1000 (page 100)\n","            try:\n","                print(\"# Now on page\", pagination + 1)\n","                results = scrape_google(keyword, \"en\")\n","                # Requests only results in English - however, the last pages tend to include results in other language\n","                for result in results:\n","                    data.append(result)\n","                    time.sleep(1.05)  # Add pauses to relieve pressure on server\n","                pagination += 1\n","                print(\"# Pausing between pages to relieve pressure on server...\")\n","                time.sleep(6)\n","            except Exception as e:\n","                print(e)\n","        print(\"# Pausing between iterations to relieve pressure on server...\")\n","        print(\"-------------------------------------------------------------\")\n","        time.sleep(30)  # For multiple keywords, use at least 30 (seconds); for a single keyword, pause is unused\n","\n","    # Format the raw data as a data frame\n","    print(\"Formatting raw data...\")\n","    data_df = pd.DataFrame(data)  # Convert list to data frame\n","    \n","    # NOTE: the author, year, journal, and publisher info are not consistently identifiable in the html or xml alone,\n","    # and so are extracted and manipulated using pandas, below\n","    \n","    # Prepare metadata and extract year\n","    data_df_extraction = data_df\n","    print(\"Cleaning data...\")\n","    for row in range(0, len(data_df_extraction['Metadata'])):\n","        # Extract pure text from the metadata html\n","        soup = BeautifulSoup(str(data_df_extraction['Metadata'][row]))\n","        # Update metadata cells with pure text extracted above\n","        data_df_extraction.at[row, 'Metadata'] = soup.get_text()\n","        # Extract year from metadata\n","        data_df_extraction['Year'] = data_df_extraction['Metadata'].str.extract('( \\d\\d\\d\\d )', expand=True)\n","    \n","    # Extract publisher info - always appears after year and a hyphen\n","    data_df_extraction = data_df_extraction.join(data_df_extraction['Metadata'].str.split('\\d\\d\\d\\d -', expand=True).rename(columns={0: 'Temp', 1: 'Publisher'}))\n","    \n","    # Extract and clean author and journal info - the indirect approach was required to handle hyphenated names\n","    data_df_extraction = data_df_extraction.join(data_df_extraction['Temp'].str.split('- ', expand=True).rename(columns={0: 'Author(s)', 1: 'Journal', 2: 'Unexpected Terms'}))\n","    data_df_extraction['Journal'] = data_df_extraction['Journal'].str.replace(',', '')  # Strip away unnecessary commas\n","    \n","    # Drop now-obsolete \"Metadata\" and \"Combined\" columns\n","    data_df_extraction = data_df_extraction.drop(columns=['Metadata', 'Temp'])\n","    \n","    # Rearrange columns into final order\n","    col = ['Keyword', 'Year', 'Page', 'Rank', 'Title', 'Author(s)', 'Description', 'Journal', 'Publisher', 'Link']\n","    data_df_clean = data_df_extraction[col]  # Create new data frame with specified column order\n","    \n","    # Export Finalized Data\n","    print(\"Exporting final data frame to Excel...\")\n","    print(\"--------------------------------------\")\n","    data_df_clean.to_excel(r'scholar_search_results_2009-2018_3.xlsx', index=None, header=True)\n","    \n","    print(\"Search complete. Results exported to Excel.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eCC13fFBnjKq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}